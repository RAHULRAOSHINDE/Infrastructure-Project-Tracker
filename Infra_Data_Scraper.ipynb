{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d522a2-eda1-477c-a49d-cbf39ffc5907",
   "metadata": {
    "id": "10d522a2-eda1-477c-a49d-cbf39ffc5907",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt install -y firefox\n",
    "!wget https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz\n",
    "!tar -xvzf geckodriver-v0.34.0-linux64.tar.gz\n",
    "!mv geckodriver /usr/local/bin/\n",
    "!pip install selenium\n",
    "!pip install geckodriver\n",
    "!pip install python_dotenv\n",
    "!pip install mysql-connector-python\n",
    "!pip install slack_sdk\n",
    "\n",
    "import os\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium import webdriver\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990143d-bbf5-45cb-b993-990bedb3be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVSaver:\n",
    "    def save_to_csv(self, data, headers,city):\n",
    "        with open(f'{city}.csv', 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(headers)\n",
    "            csv_writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb9b46-8072-4ed2-a6d1-a5ae25c770f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBHandler:\n",
    "    def __init__(self):\n",
    "        load_dotenv('/root/Desktop/infra/cred.env')\n",
    "        db_host=os.environ.get('DB_HOST')\n",
    "        db_user=os.environ.get('DB_USER')\n",
    "        db_password=os.environ.get('DB_PASSWORD')\n",
    "        self.mydb=mysql.connector.connect(\n",
    "            host=db_host,\n",
    "            user=db_user,\n",
    "            password=db_password\n",
    "        )\n",
    "        \n",
    "    def insert_data(self,city):\n",
    "        mycursor = self.mydb.cursor()\n",
    "        with open(f'modified_{city}.csv') as file:\n",
    "            rows=csv.reader(file)\n",
    "            next(rows)\n",
    "            val=[tuple(row) for row in rows]\n",
    "        query=f\"INSERT INTO {city}_source (original_id,aug_id,country_name,country_code,map_coordinates,url,region_name,region_code,title,description,status,stages,published_date,procurement_method,budget,currency,buyer,sector,subsector,location) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "        mycursor.execute(\"USE infraDB\")\n",
    "        mycursor.executemany(query,val)\n",
    "        self.mydb.commit()\n",
    "\n",
    "    def retrieve_data(self,data,city):\n",
    "        if (data==\"cdc\"):\n",
    "            #fetch the updated records from the cdc\n",
    "            cdc=pd.read_sql(f\"SELECT*FROM {city}_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "            return cdc\n",
    "        elif (data==\"projects\"):\n",
    "            #counting the number of projects\n",
    "            new_projects=pd.read_sql(f\"SELECT COUNT(*) AS total_projects FROM {city}_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "            return new_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ade3ce-4ab8-4cf8-8f7b-c43e8810229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectNotifier:\n",
    "    def __init__(self):\n",
    "        load_dotenv('/root/Desktop/infra/cred.env')\n",
    "        self.channel_name=os.environ.get('CHANNEL_NAME')\n",
    "        self.channel_id=os.environ.get('CHANNEL_ID')\n",
    "        self.user=os.environ.get('USER')\n",
    "        self.slack_token=os.environ.get('SLACK_BOT_TOKEN')\n",
    "\n",
    "    def send_alerts(self,db_handler,city):\n",
    "        #intialize webclient instance with OAtuh token\n",
    "        client=WebClient(token=self.slack_token)\n",
    "        # #fetch the updated records from the cdc\n",
    "        cdc=db_handler.retrieve_data('cdc',city)\n",
    "        #create a csv filename which accumulates updated cdc records \n",
    "        filename=f\"{city}_projects\"+\"_\"+str(datetime.now())+\".csv\"\n",
    "        cdc.to_csv(filename,index=False)\n",
    "        #counting the number of projects\n",
    "        new_projects=db_handler.retrieve_data('projects',city)\n",
    "        project_count=str(int(new_projects.iloc[0]))\n",
    "        #customization of notification that has to be sent to slack channel\n",
    "        notification_msg=\"Update:\"+\" \"+project_count+\" \"+\"infrastructure projects indentified in\"+\" \"+city\n",
    "        try:\n",
    "            client.chat_postMessage(channel=self.channel_name,text=notification_msg,username=self.user)\n",
    "            client.files_upload_v2(channel=self.channel_id,file=filename)\n",
    "        except SlackApiError as e:\n",
    "            print(f\"Error in sending message:{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EYnPo-Q0o24f",
   "metadata": {
    "id": "EYnPo-Q0o24f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Extracting sanrafael data\n",
    "\n",
    "class SanRafaelDataExtractor:\n",
    "    def __init__(self):\n",
    "        self.options = webdriver.FirefoxOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.driver = webdriver.Firefox(options=self.options)\n",
    "        load_dotenv('/root/Desktop/infra/cred.env')\n",
    "        self.sanrafael_url=os.environ.get('SANRAFAEL_URL')\n",
    "        \n",
    "    def extract_data(self):\n",
    "        # Get the webpage\n",
    "        self.driver.get(self.sanrafael_url)\n",
    "        self.driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "        sanrafael_table = self.driver.find_element('css selector', 'tbody')\n",
    "        sanrafael_rows = sanrafael_table.find_elements('css selector', \"tr\")\n",
    "        data = []\n",
    "        for row in sanrafael_rows[1:]:\n",
    "            cells = row.find_elements('css selector', \"td\")\n",
    "            row_data = [cell.text for cell in cells]\n",
    "            data.append(row_data)\n",
    "        return data\n",
    "\n",
    "    def cleanup_data(self, data):\n",
    "        #Create an empty dataframe with the desired columns\n",
    "        sanrafael_df = pd.DataFrame(columns=['original_id','aug_id','country_name','country_code','map_coordinates','url','region_name','region_code','title', 'description', 'status','stages','date','procurement_method','budget','currency','buyer','sector','subsector','location'])\n",
    "        sanrafael_csv = pd.DataFrame(data, columns=['title', 'description', 'floor area', 'Number Units', 'BMR Units', 'applicant', 'staff', 'status'])\n",
    "        sanrafael_csv.drop(['floor area', 'Number Units', 'BMR Units', 'applicant', 'staff'], axis=1, inplace=True)\n",
    "        #assigning uuid for each row\n",
    "        for _ in range(len(sanrafael_csv)):\n",
    "            sanrafael_df['aug_id']=[str(uuid.uuid4()) for _ in range(len(sanrafael_csv))]\n",
    "            sanrafael_df['url']=[self.sanrafael_url for _ in range(len(sanrafael_csv))]\n",
    "        columns = ['title','description','status']\n",
    "        for column in columns:\n",
    "            sanrafael_df[column]=sanrafael_csv[column]\n",
    "        return sanrafael_df\n",
    "\n",
    "    def close_driver(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    headers = ['title','description','floor area','Number Units','BMR Units','applicant','staff','status']\n",
    "    sanrafael_extractor = SanRafaelDataExtractor()\n",
    "    csv_saver=CSVSaver()\n",
    "    db_handler=DBHandler()\n",
    "    project_alerts=ProjectNotifier()\n",
    "    sanrafael_data = sanrafael_extractor.extract_data()\n",
    "    sanrafael_extractor.close_driver()\n",
    "    csv_saver.save_to_csv(sanrafael_data,headers,'sanrafael')\n",
    "    modified_data = sanrafael_extractor.cleanup_data(sanrafael_data)\n",
    "    modified_data.to_csv('modified_sanrafael.csv', index=False, na_rep='NULL')\n",
    "    db_handler.insert_data('sanrafael')\n",
    "    project_alerts.send_alerts(db_handler,'sanrafael')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W449wzApE5a5",
   "metadata": {
    "id": "W449wzApE5a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting Fairfield data\n",
    "\n",
    "class FairFieldDataExtractor:\n",
    "  def __init__(self):\n",
    "    self.options=webdriver.FirefoxOptions()\n",
    "    self.options.add_argument(\"--headless\")\n",
    "    self.driver = webdriver.Firefox(options=self.options)\n",
    "    load_dotenv('/root/Desktop/infra/cred.env')\n",
    "    self.fairfield_url=os.environ.get('FAIRFIELD_URL')\n",
    "  \n",
    "  def extract_data(self):\n",
    "    # Get the webpage\n",
    "    self.driver.get(self.fairfield_url)\n",
    "    # Close the pop-up\n",
    "    self.driver.find_element('css selector', '.prefix-overlay-close.prefix-overlay-action-later').click()\n",
    "    # Scroll down by 500 pixels\n",
    "    self.driver.execute_script(\"window.scrollBy(0,500)\")\n",
    "    # Find the table using CSS Selector\n",
    "    fairfield_table = self.driver.find_element('css selector', \"tbody\")\n",
    "    # Get all the rows from the table using find element\n",
    "    fairfield_rows = fairfield_table.find_elements('css selector', \"tr\")\n",
    "    fair_data=[]\n",
    "    # Iterate through the rows and extract data\n",
    "    for row in fairfield_rows:\n",
    "      # Get all cells present in the current row\n",
    "      cells = row.find_elements('css selector', \"td\")\n",
    "      # Extract and write data from each cell to csv file\n",
    "      row_data = [cell.text for cell in cells]\n",
    "      fair_data.append(row_data)\n",
    "    return fair_data\n",
    "\n",
    "  def cleanup_data(self,data):\n",
    "    #Create an empty dataframe with the desired columns\n",
    "    fairfield_df = pd.DataFrame(columns=['original_id','aug_id','country_name','country_code','map_coordinates','url','region_name','region_code','title', 'description', 'status','stages','date','procurementMethod','budget','currency','buyer','sector','subsector'])\n",
    "    #read the csv file to dataframe\n",
    "    fairfield_csv = pd.read_csv('fairfield.csv')\n",
    "    #drop the unnecessary rows\n",
    "    fairfield_csv.drop([0,1],axis=0,inplace=True)\n",
    "   #assigning uuid and url for each row\n",
    "    for _ in range(len(fairfield_csv)):\n",
    "        fairfield_df['aug_id']=[str(uuid.uuid4()) for _ in range(len(fairfield_csv))]\n",
    "        fairfield_df['url']=[self.fairfield_url for _ in range(len(fairfield_csv))]\n",
    "    #mapping columns\n",
    "    columns = ['original_id','title','location','subsector']\n",
    "    for column in columns:\n",
    "      fairfield_df[column]=fairfield_csv[column]\n",
    "    return fairfield_df\n",
    "      \n",
    "\n",
    "  def close_driver(self):\n",
    "    self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    headers = ['original_id','title','location','subsector']\n",
    "    fairfield_extractor = FairFieldDataExtractor()\n",
    "    csv_saver=CSVSaver()\n",
    "    db_handler=DBHandler()\n",
    "    project_alerts=ProjectNotifier()\n",
    "    fairfield_data = fairfield_extractor.extract_data()\n",
    "    fairfield_extractor.close_driver()\n",
    "    csv_saver.save_to_csv(fairfield_data,headers,'fairfield')\n",
    "    modified_data = fairfield_extractor.cleanup_data(fairfield_data)\n",
    "    modified_data.to_csv('modified_fairfield.csv', index=False, na_rep='NULL')\n",
    "    db_handler.insert_data('fairfield')\n",
    "    project_alerts.send_alerts(db_handler,'fairfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mvqr2gJTShCW",
   "metadata": {
    "id": "mvqr2gJTShCW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extracting elk\n",
    "class ElkGroveDataExtractor:\n",
    "  def __init__(self):\n",
    "    self.options= webdriver.FirefoxOptions()\n",
    "    self.options.add_argument(\"--headless\")\n",
    "    self.driver= webdriver.Firefox(options =self.options)\n",
    "    self.elk_url=os.environ.get('ELK_URL')\n",
    "    load_dotenv('/root/Desktop/infra/cred.env')\n",
    "    \n",
    "  def extract_data(self):\n",
    "    # Get the webpage\n",
    "    self.driver.get(self.elk_url)\n",
    "    #Get all rows\n",
    "    elk_rows=self.driver.find_elements('css selector',\"tr\")\n",
    "    elk_data = []\n",
    "    for row in elk_rows:\n",
    "      # Get all the cells present in the row\n",
    "      cells = row.find_elements('css selector', \"td\")\n",
    "      # Extract each cell and write the data to csv\n",
    "      row_data = [cell.text for cell in cells]\n",
    "      elk_data.append(row_data)\n",
    "    return elk_data\n",
    "\n",
    "\n",
    "  def cleanup_data(self,data):\n",
    "    # Create an empty dataframe with the desired columns\n",
    "    elk_df = pd.DataFrame(columns=['original_id','aug_id','country_name','country_code','map_coordinates','url','region_name','region_code','title', 'description', 'status','stages','date','procurement_method','budget','currency','buyer','sector','subsector','location'])\n",
    "    #read the csv file to dataframe\n",
    "    elk_csv = pd.read_csv('elk.csv')\n",
    "    #assigning uuid for each row\n",
    "    for _ in range(len(elk_csv)):\n",
    "        elk_df['aug_id'] = [str(uuid.uuid4()) for _ in range(len(elk_csv))]\n",
    "        elk_df['url'] = [self.elk_url for _ in range(len(elk_csv))]\n",
    "    #mapping columns\n",
    "    mapping_columns = ['title','description','status']\n",
    "    for column in mapping_columns:\n",
    "      elk_df[column] = elk_csv[column]\n",
    "    return elk_df\n",
    "\n",
    "\n",
    "  def close_driver(self):\n",
    "    self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  headers = ['project_code','title','description','applicant','status','project_materials']\n",
    "  elk_extractor = ElkGroveDataExtractor()\n",
    "  csv_saver=CSVSaver()\n",
    "  db_handler=DBHandler()\n",
    "  project_alerts=ProjectNotifier()\n",
    "  elk_data = elk_extractor.extract_data()\n",
    "  elk_extractor.close_driver()\n",
    "  csv_saver.save_to_csv(elk_data,headers,'elk')\n",
    "  modified_data = elk_extractor.cleanup_data(fairfield_data)\n",
    "  modified_data.to_csv('modified_elk.csv', index=False, na_rep='NULL')\n",
    "  db_handler.insert_data('elk')\n",
    "  project_alerts.send_alerts(db_handler,'elk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vh8DzGqOA7Bg",
   "metadata": {
    "id": "Vh8DzGqOA7Bg"
   },
   "outputs": [],
   "source": [
    "#eracting arcata data\n",
    "\n",
    "class ArcataDataExtractor:\n",
    "  def __init__(self):\n",
    "      self.options = webdriver.FirefoxOptions()\n",
    "      self.options.add_argument(\"--headless\")\n",
    "      self.driver = webdriver.Firefox(options=self.options)\n",
    "      load_dotenv('/root/Desktop/infra/cred.env')\n",
    "      self.arcata_url=os.environ.get('ARCATA_URL')\n",
    "\n",
    "  def extract_data(self):\n",
    "      # Create an empty dataframe with the desired columns\n",
    "      arcata_df = pd.DataFrame(columns=['original_id', 'aug_id', 'country_name', 'country_code', 'map_coordinates', 'url', 'region_name', 'region_code', 'title', 'description', 'status', 'stages', 'date', 'procurement_method', 'budget', 'currency', 'buyer', 'sector', 'subsector','location'])\n",
    "      # get the webpage\n",
    "      self.driver.execute_script(f\"location.href='{self.arcata_url}';\")\n",
    "      # self.driver.implicitly_wait(10)\n",
    "      headline=WebDriverWait(self.driver,30).until(\n",
    "          EC.presence_of_element_located((By.CSS_SELECTOR,\"#versionHeadLine\"))\n",
    "      )\n",
    "      # find the table with CSS selector\n",
    "      arcata_table = self.driver.find_element('css selector', \"div[class='widgetBody'] table\")\n",
    "      # Get all the rows from the table\n",
    "      arcata_rows = arcata_table.find_elements('css selector', \"tr\")\n",
    "      # Iterate through the row and accumulate title, description, status\n",
    "      for i in range(1, len(arcata_rows)):\n",
    "          # Scroll down the page by 100 pixels\n",
    "          self.driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "          # extract the project title\n",
    "          project_title = self.driver.find_element('css selector', f'tbody tr:nth-child({i}) td:nth-child(1) strong:nth-child(1)')\n",
    "          # extract the description\n",
    "          project_description = self.driver.find_element('css selector', f'tbody tr:nth-child({i}) td:nth-child(2)')\n",
    "          # extract the status\n",
    "          project_status = self.driver.find_element('css selector', f'tbody tr:nth-child({i}) td:nth-child(3)')\n",
    "          # regex pattern to extract budget\n",
    "          budget_pattern = r'\\$(\\d+(?:,\\d{3})*(?:\\.\\d+)?)(?:\\s*(million|billion|thousand))?'\n",
    "          # Search for the pattern in the text\n",
    "          bud_match = re.search(budget_pattern, project_description.text)\n",
    "          if bud_match:\n",
    "              project_budget = bud_match.group(0)\n",
    "          else:\n",
    "              project_budget = \"null\"\n",
    "          # regex pattern to extract buyer\n",
    "          buyer_pattern = r'contracted with\\s+([^\\d.,;]+)\\b'\n",
    "          # Find matches in the text\n",
    "          buy_match = re.search(buyer_pattern, project_description.text)\n",
    "          # Print the matches\n",
    "          if buy_match:\n",
    "              project_buyer = buy_match.group(0)\n",
    "          else:\n",
    "              project_buyer = \"null\"\n",
    "          arcata_df = arcata_df._append({\"title\": project_title.text, \"description\": project_description.text,\n",
    "                                          \"status\": project_status.text, \"budget\": project_budget,\"url\":self.arcata_url,\n",
    "                                          \"buyer\": project_buyer}, ignore_index=True)\n",
    "      # Replace empty values with NaN\n",
    "      arcata_df = arcata_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "      #assiging uuid for each row\n",
    "      arcata_df['aug_id'] = [str(uuid.uuid4()) for _ in range(len(arcata_df))]\n",
    "      return arcata_df\n",
    "      \n",
    "\n",
    "  def close_driver(self):\n",
    "      self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  arcata_extractor = ArcataDataExtractor()\n",
    "  db_handler=DBHandler()\n",
    "  project_alerts=ProjectNotifier()\n",
    "  arcata_data = arcata_extractor.extract_data()\n",
    "  arcata_extractor.close_driver()\n",
    "  arcata_data.to_csv('modified_arcata.csv', index=False, na_rep='NULL')\n",
    "  db_handler.insert_data('arcata')\n",
    "  project_alerts.send_alerts(db_handler,'arcata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b63991-764e-4987-8e46-d016151b4573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
