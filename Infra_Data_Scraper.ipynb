{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d522a2-eda1-477c-a49d-cbf39ffc5907",
   "metadata": {
    "id": "10d522a2-eda1-477c-a49d-cbf39ffc5907",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt install -y firefox\n",
    "!wget https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz\n",
    "!tar -xvzf geckodriver-v0.34.0-linux64.tar.gz\n",
    "!mv geckodriver /usr/local/bin/\n",
    "!pip install selenium\n",
    "!pip install geckodriver\n",
    "!pip install python_dotenv\n",
    "!pip install mysql-connector-python\n",
    "!pip install slack_sdk\n",
    "\n",
    "import os\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium import webdriver\n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk.errors import SlackApiError\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EYnPo-Q0o24f",
   "metadata": {
    "id": "EYnPo-Q0o24f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Extracting sanrafael data\n",
    "\n",
    "class SanRafaelDataExtractor:\n",
    "    def __init__(self):\n",
    "        self.options = webdriver.FirefoxOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.driver = webdriver.Firefox(options=self.options)\n",
    "        load_dotenv('/root/Desktop/infra/cred.env')\n",
    "        db_host=os.environ.get('DB_HOST')\n",
    "        db_user=os.environ.get('DB_USER')\n",
    "        db_password=os.environ.get('DB_PASSWORD')\n",
    "        self.sanrafael_url=os.environ.get('SANRAFAEL_URL')\n",
    "        self.mydb=mysql.connector.connect(\n",
    "            host=db_host,\n",
    "            user=db_user,\n",
    "            password=db_password\n",
    "        )\n",
    "\n",
    "    def extract_data(self):\n",
    "        # Get the webpage\n",
    "        self.driver.get(self.sanrafael_url)\n",
    "        self.driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "        sanrafael_table = self.driver.find_element('css selector', 'tbody')\n",
    "        sanrafael_rows = sanrafael_table.find_elements('css selector', \"tr\")\n",
    "        data = []\n",
    "        for row in sanrafael_rows[1:]:\n",
    "            cells = row.find_elements('css selector', \"td\")\n",
    "            row_data = [cell.text for cell in cells]\n",
    "            data.append(row_data)\n",
    "        return data\n",
    "\n",
    "    def save_to_csv(self, data, headers):\n",
    "        with open('sanrafael.csv', 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(headers)\n",
    "            csv_writer.writerows(data)\n",
    "\n",
    "    def cleanup_data(self, data):\n",
    "        #Create an empty dataframe with the desired columns\n",
    "        sanrafael_df = pd.DataFrame(columns=['original_id','aug_id','country_name','country_code','map_coordinates','url','region_name','region_code','title', 'description', 'status','stages','date','procurement_method','budget','currency','buyer','sector','subsector','location'])\n",
    "        sanrafael_csv = pd.DataFrame(data, columns=['title', 'description', 'floor area', 'Number Units', 'BMR Units', 'applicant', 'staff', 'status'])\n",
    "        sanrafael_csv.drop(['floor area', 'Number Units', 'BMR Units', 'applicant', 'staff'], axis=1, inplace=True)\n",
    "        #assigning uuid for each row\n",
    "        for _ in range(len(sanrafael_csv)):\n",
    "            sanrafael_df['aug_id']=[str(uuid.uuid4()) for _ in range(len(sanrafael_csv))]\n",
    "            sanrafael_df['url']=[self.sanrafael_url for _ in range(len(sanrafael_csv))]\n",
    "        columns = ['title','description','status']\n",
    "        for column in columns:\n",
    "            sanrafael_df[column]=sanrafael_csv[column]\n",
    "        return sanrafael_df\n",
    "\n",
    "    def close_driver(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "    def insert_data(self):\n",
    "        mycursor = self.mydb.cursor()\n",
    "        with open('modified_sanrafael.csv') as file:\n",
    "            rows=csv.reader(file)\n",
    "            next(rows)\n",
    "            val=[tuple(row) for row in rows]\n",
    "        query=\"INSERT INTO sanrafael_source (original_id,aug_id,country_name,country_code,map_coordinates,url,region_name,region_code,title,description,status,stages,published_date,procurement_method,budget,currency,buyer,sector,subsector,location) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "        mycursor.execute(\"USE infraDB\")\n",
    "        mycursor.executemany(query,val)\n",
    "        self.mydb.commit()\n",
    "\n",
    "    def send_alerts(self):\n",
    "        slack_token=os.environ.get('SLACK_BOT_TOKEN')\n",
    "        #intialize webclient instance with OAtuh token\n",
    "        client=WebClient(token=slack_token)\n",
    "        #fetch the updated records from the cdc\n",
    "        sanrafael_cdc=pd.read_sql(\"SELECT*FROM sanrafael_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "        #create a csv filename which accumulates updated cdc records \n",
    "        filename=\"sanrafael_projects\"+\"_\"+str(datetime.now())+\".csv\"\n",
    "        sanrafael_cdc.to_csv(filename,index=False)\n",
    "        #counting the number of projects\n",
    "        new_projects=pd.read_sql(\"SELECT COUNT(*) AS total_projects FROM sanrafael_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "        project_count=str(int(new_projects.iloc[0]))\n",
    "        #customization of notification that has to be sent to slack channel\n",
    "        notification= \"Update:\"+\" \"+project_count+\" \"+\"infrastructure projects indentified in San Rafael\"\n",
    "        try:\n",
    "            client.chat_postMessage(channel=\"california_infra_projects\",text=notification,username=\"infra_alerts\")\n",
    "            client.files_upload_v2(channel=\"C07AH45FZDZ\",file=filename)\n",
    "        except SlackApiError as e:\n",
    "            print(f\"Error in sending message:{e}\")\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    headers = ['title','description','floor area','Number Units','BMR Units','applicant','staff','status']\n",
    "    sanrafael_extractor = SanRafaelDataExtractor()\n",
    "    data = sanrafael_extractor.extract_data()\n",
    "    sanrafael_extractor.save_to_csv(data, headers)\n",
    "    modified_data = sanrafael_extractor.cleanup_data(data)\n",
    "    modified_data.to_csv('modified_sanrafael.csv', index=False, na_rep='NULL')\n",
    "    sanrafael_extractor.close_driver()\n",
    "    sanrafael_extractor.insert_data()\n",
    "    sanrafael_extractor.send_alerts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W449wzApE5a5",
   "metadata": {
    "id": "W449wzApE5a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting Fairfield data\n",
    "\n",
    "class FairFieldDataExtractor:\n",
    "  def __init__(self):\n",
    "    self.options=webdriver.FirefoxOptions()\n",
    "    self.options.add_argument(\"--headless\")\n",
    "    self.driver = webdriver.Firefox(options=self.options)\n",
    "    load_dotenv('/root/Desktop/infra/cred.env')\n",
    "    db_host=os.environ.get('DB_HOST')\n",
    "    db_user=os.environ.get('DB_USER')\n",
    "    db_password=os.environ.get('DB_PASSWORD')\n",
    "    self.fairfield_url=os.environ.get('FAIRFIELD_URL')\n",
    "    self.mydb=mysql.connector.connect(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "        )\n",
    "\n",
    "  def extract_data(self):\n",
    "    # Get the webpage\n",
    "    self.driver.get(self.fairfield_url)\n",
    "    # Close the pop-up\n",
    "    self.driver.find_element('css selector', '.prefix-overlay-close.prefix-overlay-action-later').click()\n",
    "    # Scroll down by 500 pixels\n",
    "    self.driver.execute_script(\"window.scrollBy(0,500)\")\n",
    "    # Find the table using CSS Selector\n",
    "    fairfield_table = self.driver.find_element('css selector', \"tbody\")\n",
    "    # Get all the rows from the table using find element\n",
    "    fairfield_rows = fairfield_table.find_elements('css selector', \"tr\")\n",
    "    fair_data=[]\n",
    "    # Iterate through the rows and extract data\n",
    "    for row in fairfield_rows:\n",
    "      # Get all cells present in the current row\n",
    "      cells = row.find_elements('css selector', \"td\")\n",
    "      # Extract and write data from each cell to csv file\n",
    "      row_data = [cell.text for cell in cells]\n",
    "      fair_data.append(row_data)\n",
    "    return fair_data\n",
    "\n",
    "  def save_to_csv(self,data,headers):\n",
    "    # Create a csv file to store results\n",
    "    with open('fairfield.csv', 'w', newline='') as csvfile:\n",
    "      csv_writer = csv.writer(csvfile)\n",
    "      csv_writer.writerow(headers)\n",
    "      csv_writer.writerows(data)\n",
    "\n",
    "  def cleanup_data(self,data):\n",
    "    #Create an empty dataframe with the desired columns\n",
    "    fairfield_df = pd.DataFrame(columns=['original_id','aug_id','country_name','country_code','map_coordinates','url','region_name','region_code','title', 'description', 'status','stages','date','procurementMethod','budget','currency','buyer','sector','subsector'])\n",
    "    #read the csv file to dataframe\n",
    "    fairfield_csv = pd.read_csv('fairfield.csv')\n",
    "    #drop the unnecessary rows\n",
    "    fairfield_csv.drop([0,1],axis=0,inplace=True)\n",
    "   #assigning uuid and url for each row\n",
    "    for _ in range(len(fairfield_csv)):\n",
    "        fairfield_df['aug_id']=[str(uuid.uuid4()) for _ in range(len(fairfield_csv))]\n",
    "        fairfield_df['url']=[self.fairfield_url for _ in range(len(fairfield_csv))]\n",
    "    #mapping columns\n",
    "    columns = ['original_id','title','location','subsector']\n",
    "    for column in columns:\n",
    "      fairfield_df[column]=fairfield_csv[column]\n",
    "    return fairfield_df\n",
    "      \n",
    "  def insert_data(self):\n",
    "      mycursor = self.mydb.cursor()\n",
    "      with open('modified_fairfield.csv') as file:\n",
    "          rows=csv.reader(file)\n",
    "          next(rows)\n",
    "          val=[tuple(row) for row in rows]\n",
    "      query=\"INSERT INTO fairfield_source (original_id,aug_id,country_name,country_code,map_coordinates,url,region_name,region_code,title,description,status,stages,published_date,procurement_method,budget,currency,buyer,sector,subsector,location)VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "      mycursor.execute(\"USE infraDB\")\n",
    "      mycursor.executemany(query,val)\n",
    "      self.mydb.commit()\n",
    "\n",
    "  def send_alerts(self):\n",
    "      slack_token=os.environ.get('SLACK_BOT_TOKEN')\n",
    "      #intialize webclient instance with OAtuh token\n",
    "      client=WebClient(token=slack_token)\n",
    "      #fetch the updated records from the cdc\n",
    "      fairfield_cdc=pd.read_sql(\"SELECT*FROM fairfield_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "      #create a csv filename which accumulates updated cdc records \n",
    "      filename=\"fairfield_projects\"+\"_\"+str(datetime.now())+\".csv\"\n",
    "      fairfield_cdc.to_csv(filename,index=False)\n",
    "      #counting the number of projects\n",
    "      total_projects=pd.read_sql(\"SELECT COUNT(*) AS total_projects FROM fairfield_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "      project_count=str(int(total_projects.iloc[0]))\n",
    "      #customization of notification that has to be sent to slack channel\n",
    "      notification=\"Update :\"+\" \"+project_count+\" \"+\"infrastructure projects indentified in Fairfield\"\n",
    "      try:\n",
    "          client.chat_postMessage(channel=\"california_infra_projects\",text=notification,username=\"infra_alerts\")\n",
    "          client.files_upload_v2(channel=\"C07AH45FZDZ\",file=filename)\n",
    "      except SlackApiError as e:\n",
    "          print(f\"Error in sending message:{e}\")\n",
    "\n",
    "  def close_driver(self):\n",
    "    self.driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    headers = ['original_id','title','location','subsector']\n",
    "    fairfield_extractor = FairFieldDataExtractor()\n",
    "    fairfield_data = fairfield_extractor.extract_data()\n",
    "    fairfield_extractor.save_to_csv(fairfield_data, headers)\n",
    "    fairfield_modified_data = fairfield_extractor.cleanup_data(fairfield_data)\n",
    "    fairfield_modified_data.to_csv('modified_fairfield.csv', index=False, na_rep='NULL')\n",
    "    fairfield_extractor.close_driver()\n",
    "    fairfield_extractor.insert_data()\n",
    "    fairfield_extractor.send_alerts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mvqr2gJTShCW",
   "metadata": {
    "id": "mvqr2gJTShCW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extracting elk\n",
    "class ElkGroveDataExtractor:\n",
    "  def __init__(self):\n",
    "    self.options= webdriver.FirefoxOptions()\n",
    "    self.options.add_argument(\"--headless\")\n",
    "    self.driver= webdriver.Firefox(options =self.options)\n",
    "    self.elk_url=os.environ.get('ELK_URL')\n",
    "    load_dotenv('/root/Desktop/infra/cred.env')\n",
    "    db_host=os.environ.get('DB_HOST')\n",
    "    db_user=os.environ.get('DB_USER')\n",
    "    db_password=os.environ.get('DB_PASSWORD')\n",
    "    self.mydb=mysql.connector.connect(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )\n",
    "\n",
    "  def extract_data(self):\n",
    "    # Get the webpage\n",
    "    self.driver.get(self.elk_url)\n",
    "    #Get all rows\n",
    "    elk_rows=self.driver.find_elements('css selector',\"tr\")\n",
    "    elk_data = []\n",
    "    for row in elk_rows:\n",
    "      # Get all the cells present in the row\n",
    "      cells = row.find_elements('css selector', \"td\")\n",
    "      # Extract each cell and write the data to csv\n",
    "      row_data = [cell.text for cell in cells]\n",
    "      elk_data.append(row_data)\n",
    "    return elk_data\n",
    "\n",
    "  def save_to_csv(self,data,headers):\n",
    "    with open('elk.csv', 'w', newline='') as csvfile:\n",
    "       csv_writer = csv.writer(csvfile)\n",
    "       self.driver.execute_script(\"window.scrollBy(0,550)\")\n",
    "       csv_writer.writerow(headers)\n",
    "       csv_writer.writerows(data)\n",
    "\n",
    "  def cleanup_data(self,data):\n",
    "    # Create an empty dataframe with the desired columns\n",
    "    elk_df = pd.DataFrame(columns=['original_id','aug_id','country_name','country_code','map_coordinates','url','region_name','region_code','title', 'description', 'status','stages','date','procurement_method','budget','currency','buyer','sector','subsector','location'])\n",
    "    #read the csv file to dataframe\n",
    "    elk_csv = pd.read_csv('elk.csv')\n",
    "    #assigning uuid for each row\n",
    "    for _ in range(len(elk_csv)):\n",
    "        elk_df['aug_id'] = [str(uuid.uuid4()) for _ in range(len(elk_csv))]\n",
    "        elk_df['url'] = [self.elk_url for _ in range(len(elk_csv))]\n",
    "    #mapping columns\n",
    "    mapping_columns = ['title','description','status']\n",
    "    for column in mapping_columns:\n",
    "      elk_df[column] = elk_csv[column]\n",
    "    return elk_df\n",
    "\n",
    "  def insert_data(self):\n",
    "        mycursor = self.mydb.cursor()\n",
    "        with open('modified_elk.csv') as file:\n",
    "            rows=csv.reader(file)\n",
    "            next(rows)\n",
    "            val=[tuple(row) for row in rows]\n",
    "        query=\"INSERT INTO elk_source (original_id,aug_id,country_name,country_code,map_coordinates,url,region_name,region_code,title,description,status,stages,published_date,procurement_method,budget,currency,buyer,sector,subsector,location)VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "        mycursor.execute(\"USE infraDB\")\n",
    "        mycursor.executemany(query,val)\n",
    "        self.mydb.commit()\n",
    "      \n",
    "  def send_alerts(self):\n",
    "      slack_token=os.environ.get('SLACK_BOT_TOKEN')\n",
    "      #intialize webclient instance with OAtuh token\n",
    "      client=WebClient(token=slack_token)\n",
    "      #fetch the updated records from the cdc\n",
    "      elk_cdc=pd.read_sql(\"SELECT*FROM elk_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "      #create a csv filename which accumulates updated cdc records \n",
    "      filename=\"elk_projects\"+\"_\"+str(datetime.now())+\".csv\"\n",
    "      elk_cdc.to_csv(filename,index=False)\n",
    "      #counting the number of projects\n",
    "      total_projects=pd.read_sql(\"SELECT COUNT(*) AS total_projects FROM elk_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "      project_count=str(int(total_projects.iloc[0]))\n",
    "      #customization of notification that has to be sent to slack channel\n",
    "      notification=\"Update :\"+\" \"+project_count+\" \"+\"infrastructure projects indentified in Elk Grove\"\n",
    "      try:\n",
    "          client.chat_postMessage(channel=\"california_infra_projects\",text=notification,username=\"infra_alerts\")\n",
    "          client.files_upload_v2(channel=\"C07AH45FZDZ\",file=filename)\n",
    "      except SlackApiError as e:\n",
    "          print(f\"Error in sending message:{e}\")\n",
    "\n",
    "  def close_driver(self):\n",
    "    self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  headers = ['project_code','title','description','applicant','status','project_materials']\n",
    "  elkgrover_extractor = ElkGroveDataExtractor()\n",
    "  elkgrover_data = elkgrover_extractor.extract_data()\n",
    "  elkgrover_extractor.save_to_csv(elkgrover_data,headers)\n",
    "  modified_data = elkgrover_extractor.cleanup_data(elkgrover_data)\n",
    "  modified_data.to_csv('modified_elk.csv',index=False,na_rep='NULL')\n",
    "  elkgrover_extractor.close_driver()\n",
    "  elkgrover_extractor.insert_data()\n",
    "  elkgrover_extractor.send_alerts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vh8DzGqOA7Bg",
   "metadata": {
    "id": "Vh8DzGqOA7Bg"
   },
   "outputs": [],
   "source": [
    "#eracting arcata data\n",
    "\n",
    "class ArcataDataExtractor:\n",
    "  def __init__(self):\n",
    "      self.options = webdriver.FirefoxOptions()\n",
    "      self.options.add_argument(\"--headless\")\n",
    "      self.driver = webdriver.Firefox(options=self.options)\n",
    "      load_dotenv('/root/Desktop/infra/cred.env')\n",
    "      self.arcata_url=os.environ.get('ARCATA_URL')\n",
    "      db_host=os.environ.get('DB_HOST')\n",
    "      db_user=os.environ.get('DB_USER')\n",
    "      db_password=os.environ.get('DB_PASSWORD')\n",
    "      self.mydb=mysql.connector.connect(\n",
    "          host=db_host,\n",
    "          user=db_user,\n",
    "          password=db_password\n",
    "        )\n",
    "\n",
    "  def extract_data(self):\n",
    "      # Create an empty dataframe with the desired columns\n",
    "      arcata_df = pd.DataFrame(columns=['original_id', 'aug_id', 'country_name', 'country_code', 'map_coordinates', 'url', 'region_name', 'region_code', 'title', 'description', 'status', 'stages', 'date', 'procurement_method', 'budget', 'currency', 'buyer', 'sector', 'subsector','location'])\n",
    "      # get the webpage\n",
    "      self.driver.execute_script(f\"location.href='{self.arcata_url}';\")\n",
    "      # self.driver.implicitly_wait(10)\n",
    "      headline=WebDriverWait(self.driver,30).until(\n",
    "          EC.presence_of_element_located((By.CSS_SELECTOR,\"#versionHeadLine\"))\n",
    "      )\n",
    "      # find the table with CSS selector\n",
    "      arcata_table = self.driver.find_element('css selector', \"div[class='widgetBody'] table\")\n",
    "      # Get all the rows from the table\n",
    "      arcata_rows = arcata_table.find_elements('css selector', \"tr\")\n",
    "      # Iterate through the row and accumulate title, description, status\n",
    "      for i in range(1, len(arcata_rows)):\n",
    "          # Scroll down the page by 100 pixels\n",
    "          self.driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "          # extract the project title\n",
    "          project_title = self.driver.find_element('css selector', f'tbody tr:nth-child({i}) td:nth-child(1) strong:nth-child(1)')\n",
    "          # extract the description\n",
    "          project_description = self.driver.find_element('css selector', f'tbody tr:nth-child({i}) td:nth-child(2)')\n",
    "          # extract the status\n",
    "          project_status = self.driver.find_element('css selector', f'tbody tr:nth-child({i}) td:nth-child(3)')\n",
    "          # regex pattern to extract budget\n",
    "          budget_pattern = r'\\$(\\d+(?:,\\d{3})*(?:\\.\\d+)?)(?:\\s*(million|billion|thousand))?'\n",
    "          # Search for the pattern in the text\n",
    "          bud_match = re.search(budget_pattern, project_description.text)\n",
    "          if bud_match:\n",
    "              project_budget = bud_match.group(0)\n",
    "          else:\n",
    "              project_budget = \"null\"\n",
    "          # regex pattern to extract buyer\n",
    "          buyer_pattern = r'contracted with\\s+([^\\d.,;]+)\\b'\n",
    "          # Find matches in the text\n",
    "          buy_match = re.search(buyer_pattern, project_description.text)\n",
    "          # Print the matches\n",
    "          if buy_match:\n",
    "              project_buyer = buy_match.group(0)\n",
    "          else:\n",
    "              project_buyer = \"null\"\n",
    "          arcata_df = arcata_df._append({\"title\": project_title.text, \"description\": project_description.text,\n",
    "                                          \"status\": project_status.text, \"budget\": project_budget,\"url\":self.arcata_url,\n",
    "                                          \"buyer\": project_buyer}, ignore_index=True)\n",
    "      # Replace empty values with NaN\n",
    "      arcata_df = arcata_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "      #assiging uuid for each row\n",
    "      arcata_df['aug_id'] = [str(uuid.uuid4()) for _ in range(len(arcata_df))]\n",
    "      return arcata_df\n",
    "      \n",
    "  def insert_data(self):\n",
    "      mycursor = self.mydb.cursor()\n",
    "      with open('modified_arcata.csv') as file:\n",
    "          rows=csv.reader(file)\n",
    "          next(rows)\n",
    "          val=[tuple(row) for row in rows]\n",
    "      query=\"INSERT INTO arcata_source (original_id,aug_id,country_name,country_code,map_coordinates,url,region_name,region_code,title,description,status,stages,published_date,procurement_method,budget,currency,buyer,sector,subsector,location)VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "      mycursor.execute(\"USE infraDB\")\n",
    "      mycursor.executemany(query,val)\n",
    "      self.mydb.commit()\n",
    "\n",
    "  def send_alerts(self):\n",
    "      slack_token=os.environ.get('SLACK_BOT_TOKEN')\n",
    "      #intialize webclient instance with OAtuh token\n",
    "      client=WebClient(token=slack_token)\n",
    "      #fetch the updated records from the cdc\n",
    "      arcata_cdc=pd.read_sql(\"SELECT*FROM arcata_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "      #create a csv filename which accumulates updated cdc records \n",
    "      filename=\"arcata_projects\"+\"_\"+str(datetime.now())+\".csv\"\n",
    "      arcata_cdc.to_csv(filename,index=False)\n",
    "      #counting the number of projects\n",
    "      total_projects=pd.read_sql(\"SELECT COUNT(*) AS total_projects FROM  arcata_cdc WHERE DATE(last_modified_dt)=DATE(NOW())\",self.mydb)\n",
    "      project_count=str(int(total_projects.iloc[0]))\n",
    "      #customization of notification that has to be sent to slack channel\n",
    "      notification=\"Update :\"+\" \"+project_count+\" \"+\"infrastructure projects indentified in Arcata\"\n",
    "      try:\n",
    "          client.chat_postMessage(channel=\"california_infra_projects\",text=notification,username=\"infra_alerts\")\n",
    "          client.files_upload_v2(channel=\"C07AH45FZDZ\",file=filename)\n",
    "      except SlackApiError as e:\n",
    "          print(f\"Error in sending message:{e}\")\n",
    "\n",
    "  def close_driver(self):\n",
    "      self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  arcata_extractor = ArcataDataExtractor()\n",
    "  arcata_data = arcata_extractor.extract_data()\n",
    "  arcata_data.to_csv('modified_arcata.csv', index=False, na_rep='NULL')\n",
    "  arcata_extractor.insert_data()\n",
    "  arcata_extractor.close_driver()\n",
    "  arcata_extractor.send_alerts()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
